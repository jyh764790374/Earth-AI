{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import netCDF4 as nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed = 427):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_enhance(df, data_type='feature'):\n",
    "    if data_type=='feature':\n",
    "        df_ = df\n",
    "        for i in range(1, 12):\n",
    "            dfi = df_.reshape(-1, 24, 72)\n",
    "            dfi = dfi[i:-(12-i), :, :]\n",
    "            dfi = dfi.reshape(-1, 12, 24, 72)\n",
    "            df = np.concatenate((df, dfi), axis=0)\n",
    "    elif data_type=='label':\n",
    "        df_ = np.concatenate((df[:, :12], df[-1:, 12:24], df[-1:, 24:36]), axis=0)\n",
    "        for i in range(1, 12):\n",
    "            df_ = df_.reshape(-1)\n",
    "            dfi1 = df_[i:-(36-i)].reshape(-1, 12)\n",
    "            dfi2 = df_[i+12:-(24-i)].reshape(-1, 12)\n",
    "            dfi3 = df_[i+24:-(12-i)].reshape(-1, 12)\n",
    "            dfi = np.concatenate((dfi1, dfi2, dfi3), axis=1)\n",
    "            df = np.concatenate((df, dfi), axis=0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data2():\n",
    "    # CMIP data    \n",
    "    train = xr.open_dataset('../data/enso_round1_train_20210201/CMIP_train.nc')\n",
    "    label = xr.open_dataset('../data/enso_round1_train_20210201/CMIP_label.nc')    \n",
    "   \n",
    "    train_sst = data_enhance(train['sst'][:, :12].values)  # (4645, 12, 24, 72)\n",
    "    train_t300 = data_enhance(train['t300'][:, :12].values)\n",
    "    train_ua = data_enhance(train['ua'][:, :12].values)\n",
    "    train_va = data_enhance(train['va'][:, :12].values)\n",
    "    train_label = data_enhance(label['nino'].values, data_type='label')\n",
    "    train_label = train_label[:, 12:36]\n",
    "\n",
    "    train_ua = np.nan_to_num(train_ua)\n",
    "    train_va = np.nan_to_num(train_va)\n",
    "    train_t300 = np.nan_to_num(train_t300)\n",
    "    train_sst = np.nan_to_num(train_sst)\n",
    "\n",
    "    # SODA data    \n",
    "    train2 = xr.open_dataset('./data/enso_round1_train_20210201/SODA_train.nc')\n",
    "    label2 = xr.open_dataset('./data/enso_round1_train_20210201/SODA_label.nc')\n",
    "    \n",
    "    train_sst2 = data_enhance(train2['sst'][:, :12].values)  # (4645, 12, 24, 72)\n",
    "    train_t3002 = data_enhance(train2['t300'][:, :12].values)\n",
    "    train_ua2 = data_enhance(train2['ua'][:, :12].values)\n",
    "    train_va2 = data_enhance(train2['va'][:, :12].values)\n",
    "    train_label2 = data_enhance(label2['nino'].values, data_type='label')\n",
    "    train_label2 = train_label2[:, 12:36]\n",
    "\n",
    "    print('Train samples: {}, Valid samples: {}'.format(len(train_label), len(train_label2)))\n",
    "\n",
    "    dict_train = {\n",
    "        'sst':train_sst,\n",
    "        't300':train_t300,\n",
    "        'ua':train_ua,\n",
    "        'va': train_va,\n",
    "        'label': train_label}\n",
    "    dict_valid = {\n",
    "        'sst':train_sst2,\n",
    "        't300':train_t3002,\n",
    "        'ua':train_ua2,\n",
    "        'va': train_va2,\n",
    "        'label': train_label2}\n",
    "    train_dataset = EarthDataSet(dict_train)\n",
    "    valid_dataset = EarthDataSet(dict_valid)\n",
    "    return train_dataset, valid_dataset\n",
    "    \n",
    "\n",
    "class EarthDataSet(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data['sst'])\n",
    "\n",
    "    def __getitem__(self, idx):   \n",
    "        return (self.data['sst'][idx], self.data['t300'][idx], self.data['ua'][idx], self.data['va'][idx]), self.data['label'][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleSpatailTimeNN(nn.Module):\n",
    "    def __init__(self, n_cnn_layer:int=1, kernals:list=[3], n_lstm_units:int=64):\n",
    "        super(simpleSpatailTimeNN, self).__init__()\n",
    "        self.conv1 = nn.ModuleList([nn.Conv2d(in_channels=12, out_channels=12, kernel_size=i) for i in kernals]) \n",
    "        self.conv2 = nn.ModuleList([nn.Conv2d(in_channels=12, out_channels=12, kernel_size=i) for i in kernals])\n",
    "        self.conv3 = nn.ModuleList([nn.Conv2d(in_channels=12, out_channels=12, kernel_size=i) for i in kernals])\n",
    "        self.conv4 = nn.ModuleList([nn.Conv2d(in_channels=12, out_channels=12, kernel_size=i) for i in kernals])\n",
    "        self.pool1 = nn.AdaptiveAvgPool2d((22, 1))\n",
    "        self.pool2 = nn.AdaptiveAvgPool2d((1, 70))\n",
    "        self.pool3 = nn.AdaptiveAvgPool2d((1, 128))\n",
    "        self.batch_norm = nn.BatchNorm1d(12, affine=False)\n",
    "        self.lstm = nn.LSTM(1540 * 4, n_lstm_units, 2, bidirectional=True)\n",
    "        self.linear = nn.Linear(128, 24)\n",
    "\n",
    "    def forward(self, sst, t300, ua, va):\n",
    "        for conv1 in self.conv1:\n",
    "            sst = conv1(sst)  # batch * 12 * (24 - 2) * (72 -2)\n",
    "        for conv2 in self.conv2:\n",
    "            t300 = conv2(t300)\n",
    "        for conv3 in self.conv3:\n",
    "            ua = conv3(ua)\n",
    "        for conv4 in self.conv4:\n",
    "            va = conv4(va)\n",
    "\n",
    "        sst = torch.flatten(sst, start_dim=2)  # batch * 12 * 1540\n",
    "        t300 = torch.flatten(t300, start_dim=2)\n",
    "        ua = torch.flatten(ua, start_dim=2)\n",
    "        va = torch.flatten(va, start_dim=2)  # if flat, lstm input_dims = 1540 * 4              \n",
    "            \n",
    "        x = torch.cat([sst, t300, ua, va], dim=-1)    \n",
    "        x = self.batch_norm(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.pool3(x).squeeze(dim=-2)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coreff(x, y):\n",
    "    x_mean = np.mean(x)\n",
    "    y_mean = np.mean(y)\n",
    "    c1 = sum((x - x_mean) * (y - y_mean))\n",
    "    c2 = sum((x - x_mean)**2) * sum((y - y_mean)**2)\n",
    "    return c1/np.sqrt(c2)\n",
    "\n",
    "def rmse(preds, y):\n",
    "    return np.sqrt(sum((preds - y)**2)/preds.shape[0])\n",
    "\n",
    "def eval_score(preds, label):\n",
    "    # preds = preds.cpu().detach().numpy().squeeze()\n",
    "    # label = label.cpu().detach().numpy().squeeze()\n",
    "    acskill = 0\n",
    "    RMSE = 0\n",
    "    a = 0\n",
    "    a = [1.5]*4 + [2]*7 + [3]*7 + [4]*6\n",
    "    for i in range(24):\n",
    "        RMSE += rmse(label[:, i], preds[:, i])\n",
    "        cor = coreff(label[:, i], preds[:, i])\n",
    "    \n",
    "        acskill += a[i] * np.log(i+1) * cor\n",
    "    return 2/3 * acskill - RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 4645, Valid samples: 100\n",
      "Train samples: 4645, Valid samples: 100\n",
      "========================\n",
      "lr:2.0000e-04\n",
      "Epoch: [0][0/37]\tLoss_mse 0.0372694\tlr: 2.0000000e-04\t\n",
      "Epoch: [0][5/37]\tLoss_mse 0.0339390\tlr: 2.0000000e-04\t\n",
      "Epoch: [0][10/37]\tLoss_mse 0.0266590\tlr: 2.0000000e-04\t\n",
      "Epoch: [0][15/37]\tLoss_mse 0.0381869\tlr: 2.0000000e-04\t\n",
      "Epoch: [0][20/37]\tLoss_mse 0.0162637\tlr: 2.0000000e-04\t\n",
      "Epoch: [0][25/37]\tLoss_mse 0.0179244\tlr: 2.0000000e-04\t\n",
      "Epoch: [0][30/37]\tLoss_mse 0.0243381\tlr: 2.0000000e-04\t\n",
      "Epoch: [0][35/37]\tLoss_mse 0.0207633\tlr: 2.0000000e-04\t\n",
      "Model saved!\n",
      "Epoch: 1, Valid Score 79.94345188131022\n",
      "Model saved!\n",
      "Model saved successfully\n",
      "========================\n",
      "lr:2.0000e-04\n",
      "Epoch: [1][0/37]\tLoss_mse 0.0292350\tlr: 2.0000000e-04\t\n",
      "Epoch: [1][5/37]\tLoss_mse 0.0339534\tlr: 2.0000000e-04\t\n",
      "Epoch: [1][10/37]\tLoss_mse 0.0261335\tlr: 2.0000000e-04\t\n",
      "Epoch: [1][15/37]\tLoss_mse 0.0377939\tlr: 2.0000000e-04\t\n",
      "Epoch: [1][20/37]\tLoss_mse 0.0168834\tlr: 2.0000000e-04\t\n",
      "Epoch: [1][25/37]\tLoss_mse 0.0189022\tlr: 2.0000000e-04\t\n",
      "Epoch: [1][30/37]\tLoss_mse 0.0248930\tlr: 2.0000000e-04\t\n",
      "Epoch: [1][35/37]\tLoss_mse 0.0266622\tlr: 2.0000000e-04\t\n",
      "Model saved!\n",
      "Epoch: 2, Valid Score 80.20561478145027\n",
      "Model saved!\n",
      "Model saved successfully\n",
      "========================\n",
      "lr:2.0000e-04\n",
      "Epoch: [2][0/37]\tLoss_mse 0.0231809\tlr: 2.0000000e-04\t\n",
      "Epoch: [2][5/37]\tLoss_mse 0.0339048\tlr: 2.0000000e-04\t\n",
      "Epoch: [2][10/37]\tLoss_mse 0.0400746\tlr: 2.0000000e-04\t\n",
      "Epoch: [2][15/37]\tLoss_mse 0.0353772\tlr: 2.0000000e-04\t\n",
      "Epoch: [2][20/37]\tLoss_mse 0.0201313\tlr: 2.0000000e-04\t\n",
      "Epoch: [2][25/37]\tLoss_mse 0.0218744\tlr: 2.0000000e-04\t\n",
      "Epoch: [2][30/37]\tLoss_mse 0.0247064\tlr: 2.0000000e-04\t\n",
      "Epoch: [2][35/37]\tLoss_mse 0.0270136\tlr: 2.0000000e-04\t\n",
      "========================\n",
      "lr:2.0000e-04\n",
      "Epoch: [3][0/37]\tLoss_mse 0.0261432\tlr: 2.0000000e-04\t\n",
      "Epoch: [3][5/37]\tLoss_mse 0.0476101\tlr: 2.0000000e-04\t\n",
      "Epoch: [3][10/37]\tLoss_mse 0.0321339\tlr: 2.0000000e-04\t\n",
      "Epoch: [3][15/37]\tLoss_mse 0.0736215\tlr: 2.0000000e-04\t\n",
      "Epoch: [3][20/37]\tLoss_mse 0.0216356\tlr: 2.0000000e-04\t\n",
      "Epoch: [3][25/37]\tLoss_mse 0.0250559\tlr: 2.0000000e-04\t\n",
      "Epoch: [3][30/37]\tLoss_mse 0.0341369\tlr: 2.0000000e-04\t\n",
      "Epoch: [3][35/37]\tLoss_mse 0.0253344\tlr: 2.0000000e-04\t\n",
      "========================\n",
      "lr:2.0000e-04\n",
      "Epoch: [4][0/37]\tLoss_mse 0.0296848\tlr: 2.0000000e-04\t\n",
      "Epoch: [4][5/37]\tLoss_mse 0.0354056\tlr: 2.0000000e-04\t\n",
      "Epoch: [4][10/37]\tLoss_mse 0.0319293\tlr: 2.0000000e-04\t\n",
      "Epoch: [4][15/37]\tLoss_mse 0.0380673\tlr: 2.0000000e-04\t\n",
      "Epoch: [4][20/37]\tLoss_mse 0.0289336\tlr: 2.0000000e-04\t\n",
      "Epoch: [4][25/37]\tLoss_mse 0.0199651\tlr: 2.0000000e-04\t\n",
      "Epoch: [4][30/37]\tLoss_mse 0.0406187\tlr: 2.0000000e-04\t\n",
      "Epoch: [4][35/37]\tLoss_mse 0.0466186\tlr: 2.0000000e-04\t\n",
      "Model saved!\n",
      "Epoch: 5, Valid Score 80.35471258196229\n",
      "Model saved!\n",
      "Model saved successfully\n",
      "========================\n",
      "lr:2.0000e-04\n",
      "Epoch: [5][0/37]\tLoss_mse 0.0631587\tlr: 2.0000000e-04\t\n",
      "Epoch: [5][5/37]\tLoss_mse 0.0324812\tlr: 2.0000000e-04\t\n",
      "Epoch: [5][10/37]\tLoss_mse 0.0353386\tlr: 2.0000000e-04\t\n",
      "Epoch: [5][15/37]\tLoss_mse 0.0424327\tlr: 2.0000000e-04\t\n",
      "Epoch: [5][20/37]\tLoss_mse 0.0249448\tlr: 2.0000000e-04\t\n",
      "Epoch: [5][25/37]\tLoss_mse 0.0221983\tlr: 2.0000000e-04\t\n",
      "Epoch: [5][30/37]\tLoss_mse 0.0342586\tlr: 2.0000000e-04\t\n",
      "Epoch: [5][35/37]\tLoss_mse 0.0354672\tlr: 2.0000000e-04\t\n",
      "========================\n",
      "lr:2.0000e-04\n",
      "Epoch: [6][0/37]\tLoss_mse 0.0541984\tlr: 1.9995312e-04\t\n",
      "Epoch: [6][5/37]\tLoss_mse 0.0549448\tlr: 1.9831729e-04\t\n",
      "Epoch: [6][10/37]\tLoss_mse 0.0312497\tlr: 1.9438367e-04\t\n",
      "Epoch: [6][15/37]\tLoss_mse 0.0575737\tlr: 1.8824913e-04\t\n",
      "Epoch: [6][20/37]\tLoss_mse 0.0285207\tlr: 1.8006473e-04\t\n",
      "Epoch: [6][25/37]\tLoss_mse 0.0220855\tlr: 1.7003198e-04\t\n",
      "Epoch: [6][30/37]\tLoss_mse 0.0275737\tlr: 1.5839792e-04\t\n",
      "Epoch: [6][35/37]\tLoss_mse 0.0393406\tlr: 1.4544903e-04\t\n",
      "========================\n",
      "lr:1.4273e-04\n",
      "Epoch: [7][0/37]\tLoss_mse 0.0630008\tlr: 1.3997183e-04\t\n",
      "Epoch: [7][5/37]\tLoss_mse 0.0758469\tlr: 1.2572361e-04\t\n",
      "Epoch: [7][10/37]\tLoss_mse 0.0721324\tlr: 1.1096510e-04\t\n",
      "Epoch: [7][15/37]\tLoss_mse 0.0740041\tlr: 9.6059710e-05\t\n",
      "Epoch: [7][20/37]\tLoss_mse 0.0368535\tlr: 8.1374461e-05\t\n",
      "Epoch: [7][25/37]\tLoss_mse 0.0602969\tlr: 6.7270950e-05\t\n",
      "Epoch: [7][30/37]\tLoss_mse 0.0593581\tlr: 5.4096454e-05\t\n",
      "Epoch: [7][35/37]\tLoss_mse 0.0534348\tlr: 4.2175373e-05\t\n",
      "========================\n",
      "lr:3.9968e-05\n",
      "Epoch: [8][0/37]\tLoss_mse 0.0897871\tlr: 3.7824856e-05\t\n",
      "Epoch: [8][5/37]\tLoss_mse 0.0752847\tlr: 2.8143386e-05\t\n",
      "Epoch: [8][10/37]\tLoss_mse 0.0544339\tlr: 2.0354380e-05\t\n",
      "Epoch: [8][15/37]\tLoss_mse 0.0527946\tlr: 1.4649631e-05\t\n",
      "Epoch: [8][20/37]\tLoss_mse 0.0168832\tlr: 1.1169608e-05\t\n",
      "Epoch: [8][25/37]\tLoss_mse 0.0203368\tlr: 1.0000000e-05\t\n",
      "Epoch: [8][30/37]\tLoss_mse 0.0252432\tlr: 1.1169608e-05\t\n",
      "Epoch: [8][35/37]\tLoss_mse 0.0247854\tlr: 1.4649631e-05\t\n",
      "========================\n",
      "lr:1.5616e-05\n",
      "Epoch: [9][0/37]\tLoss_mse 0.0436461\tlr: 1.6671234e-05\t\n",
      "Epoch: [9][5/37]\tLoss_mse 0.0414291\tlr: 2.3229507e-05\t\n",
      "Epoch: [9][10/37]\tLoss_mse 0.0445653\tlr: 3.1801242e-05\t\n",
      "Epoch: [9][15/37]\tLoss_mse 0.0483530\tlr: 4.2175373e-05\t\n",
      "Epoch: [9][20/37]\tLoss_mse 0.0162465\tlr: 5.4096454e-05\t\n",
      "Epoch: [9][25/37]\tLoss_mse 0.0228101\tlr: 6.7270950e-05\t\n",
      "Epoch: [9][30/37]\tLoss_mse 0.0292836\tlr: 8.1374461e-05\t\n",
      "Epoch: [9][35/37]\tLoss_mse 0.0250647\tlr: 9.6059710e-05\t\n",
      "========================\n",
      "lr:9.9035e-05\n",
      "Epoch: [10][0/37]\tLoss_mse 0.0364236\tlr: 1.0201598e-04\t\n",
      "Epoch: [10][5/37]\tLoss_mse 0.0290254\tlr: 1.1690666e-04\t\n",
      "Epoch: [10][10/37]\tLoss_mse 0.0476043\tlr: 1.3150416e-04\t\n",
      "Epoch: [10][15/37]\tLoss_mse 0.0399589\tlr: 1.4544903e-04\t\n",
      "Epoch: [10][20/37]\tLoss_mse 0.0165355\tlr: 1.5839792e-04\t\n",
      "Epoch: [10][25/37]\tLoss_mse 0.0288066\tlr: 1.7003198e-04\t\n",
      "Epoch: [10][30/37]\tLoss_mse 0.0309239\tlr: 1.8006473e-04\t\n",
      "Epoch: [10][35/37]\tLoss_mse 0.0204409\tlr: 1.8824913e-04\t\n",
      "========================\n",
      "lr:1.8965e-04\n",
      "Epoch: [11][0/37]\tLoss_mse 0.0286580\tlr: 1.9095857e-04\t\n",
      "Epoch: [11][5/37]\tLoss_mse 0.0264785\tlr: 1.9622790e-04\t\n",
      "Epoch: [11][10/37]\tLoss_mse 0.0412069\tlr: 1.9925090e-04\t\n",
      "Epoch: [11][15/37]\tLoss_mse 0.0311573\tlr: 1.9995312e-04\t\n",
      "Epoch: [11][20/37]\tLoss_mse 0.0140425\tlr: 1.9831729e-04\t\n",
      "Epoch: [11][25/37]\tLoss_mse 0.0197875\tlr: 1.9438367e-04\t\n",
      "Epoch: [11][30/37]\tLoss_mse 0.0240852\tlr: 1.8824913e-04\t\n",
      "Epoch: [11][35/37]\tLoss_mse 0.0174209\tlr: 1.8006473e-04\t\n",
      "========================\n",
      "lr:1.7820e-04\n",
      "Epoch: [12][0/37]\tLoss_mse 0.0348892\tlr: 1.7626055e-04\t\n",
      "Epoch: [12][5/37]\tLoss_mse 0.0264898\tlr: 1.6555528e-04\t\n",
      "Epoch: [12][10/37]\tLoss_mse 0.0347173\tlr: 1.5335893e-04\t\n",
      "Epoch: [12][15/37]\tLoss_mse 0.0364212\tlr: 1.3997183e-04\t\n",
      "Epoch: [12][20/37]\tLoss_mse 0.0152500\tlr: 1.2572361e-04\t\n",
      "Epoch: [12][25/37]\tLoss_mse 0.0143693\tlr: 1.1096510e-04\t\n",
      "Epoch: [12][30/37]\tLoss_mse 0.0210889\tlr: 9.6059710e-05\t\n",
      "Epoch: [12][35/37]\tLoss_mse 0.0214039\tlr: 8.1374461e-05\t\n",
      "========================\n",
      "lr:7.8496e-05\n",
      "Epoch: [13][0/37]\tLoss_mse 0.0421082\tlr: 7.5643386e-05\t\n",
      "Epoch: [13][5/37]\tLoss_mse 0.0426251\tlr: 6.1870903e-05\t\n",
      "Epoch: [13][10/37]\tLoss_mse 0.0224545\tlr: 4.9160401e-05\t\n",
      "Epoch: [13][15/37]\tLoss_mse 0.0352571\tlr: 3.7824856e-05\t\n",
      "Epoch: [13][20/37]\tLoss_mse 0.0170995\tlr: 2.8143386e-05\t\n",
      "Epoch: [13][25/37]\tLoss_mse 0.0158703\tlr: 2.0354380e-05\t\n",
      "Epoch: [13][30/37]\tLoss_mse 0.0157902\tlr: 1.4649631e-05\t\n",
      "Epoch: [13][35/37]\tLoss_mse 0.0145614\tlr: 1.1169608e-05\t\n",
      "========================\n",
      "lr:1.0749e-05\n",
      "Epoch: [14][0/37]\tLoss_mse 0.0163668\tlr: 1.0421613e-05\t\n",
      "Epoch: [14][5/37]\tLoss_mse 0.0228396\tlr: 1.0187461e-05\t\n",
      "Epoch: [14][10/37]\tLoss_mse 0.0186367\tlr: 1.2287908e-05\t\n",
      "Epoch: [14][15/37]\tLoss_mse 0.0259897\tlr: 1.6671234e-05\t\n",
      "Epoch: [14][20/37]\tLoss_mse 0.0130939\tlr: 2.3229507e-05\t\n",
      "Epoch: [14][25/37]\tLoss_mse 0.0128643\tlr: 3.1801242e-05\t\n",
      "Epoch: [14][30/37]\tLoss_mse 0.0145218\tlr: 4.2175373e-05\t\n",
      "Epoch: [14][35/37]\tLoss_mse 0.0141575\tlr: 5.4096454e-05\t\n",
      "========================\n",
      "lr:5.6641e-05\n",
      "Epoch: [15][0/37]\tLoss_mse 0.0160457\tlr: 5.9233401e-05\t\n",
      "Epoch: [15][5/37]\tLoss_mse 0.0246096\tlr: 7.2819898e-05\t\n",
      "Epoch: [15][10/37]\tLoss_mse 0.0182007\tlr: 8.7198775e-05\t\n",
      "Epoch: [15][15/37]\tLoss_mse 0.0262847\tlr: 1.0201598e-04\t\n",
      "Epoch: [15][20/37]\tLoss_mse 0.0121638\tlr: 1.1690666e-04\t\n",
      "Epoch: [15][25/37]\tLoss_mse 0.0115920\tlr: 1.3150416e-04\t\n",
      "Epoch: [15][30/37]\tLoss_mse 0.0151897\tlr: 1.4544903e-04\t\n",
      "Epoch: [15][35/37]\tLoss_mse 0.0143777\tlr: 1.5839792e-04\t\n",
      "========================\n",
      "lr:1.6084e-04\n",
      "Epoch: [16][0/37]\tLoss_mse 0.0169308\tlr: 1.6322617e-04\t\n",
      "Epoch: [16][5/37]\tLoss_mse 0.0204958\tlr: 1.7425202e-04\t\n",
      "Epoch: [16][10/37]\tLoss_mse 0.0178334\tlr: 1.8357265e-04\t\n",
      "Epoch: [16][15/37]\tLoss_mse 0.0234429\tlr: 1.9095857e-04\t\n",
      "Epoch: [16][20/37]\tLoss_mse 0.0105897\tlr: 1.9622790e-04\t\n",
      "Epoch: [16][25/37]\tLoss_mse 0.0110107\tlr: 1.9925090e-04\t\n",
      "Epoch: [16][30/37]\tLoss_mse 0.0145428\tlr: 1.9995312e-04\t\n",
      "Epoch: [16][35/37]\tLoss_mse 0.0124085\tlr: 1.9831729e-04\t\n",
      "========================\n",
      "lr:1.9771e-04\n",
      "Epoch: [17][0/37]\tLoss_mse 0.0166885\tlr: 1.9701540e-04\t\n",
      "Epoch: [17][5/37]\tLoss_mse 0.0192507\tlr: 1.9218669e-04\t\n",
      "Epoch: [17][10/37]\tLoss_mse 0.0171648\tlr: 1.8521115e-04\t\n",
      "Epoch: [17][15/37]\tLoss_mse 0.0228574\tlr: 1.7626055e-04\t\n",
      "Epoch: [17][20/37]\tLoss_mse 0.0101625\tlr: 1.6555528e-04\t\n",
      "Epoch: [17][25/37]\tLoss_mse 0.0102349\tlr: 1.5335893e-04\t\n",
      "Epoch: [17][30/37]\tLoss_mse 0.0132675\tlr: 1.3997183e-04\t\n",
      "Epoch: [17][35/37]\tLoss_mse 0.0119974\tlr: 1.2572361e-04\t\n",
      "========================\n",
      "lr:1.2280e-04\n",
      "Epoch: [18][0/37]\tLoss_mse 0.0166085\tlr: 1.1986127e-04\t\n",
      "Epoch: [18][5/37]\tLoss_mse 0.0206477\tlr: 1.0500000e-04\t\n",
      "Epoch: [18][10/37]\tLoss_mse 0.0159713\tlr: 9.0138726e-05\t\n",
      "Epoch: [18][15/37]\tLoss_mse 0.0221926\tlr: 7.5643386e-05\t\n",
      "Epoch: [18][20/37]\tLoss_mse 0.0112826\tlr: 6.1870903e-05\t\n",
      "Epoch: [18][25/37]\tLoss_mse 0.0119720\tlr: 4.9160401e-05\t\n",
      "Epoch: [18][30/37]\tLoss_mse 0.0131566\tlr: 3.7824856e-05\t\n",
      "Epoch: [18][35/37]\tLoss_mse 0.0123145\tlr: 2.8143386e-05\t\n",
      "========================\n",
      "lr:2.6427e-05\n",
      "Epoch: [19][0/37]\tLoss_mse 0.0133993\tlr: 2.4788847e-05\t\n",
      "Epoch: [19][5/37]\tLoss_mse 0.0189838\tlr: 1.7813311e-05\t\n",
      "Epoch: [19][10/37]\tLoss_mse 0.0161438\tlr: 1.2984600e-05\t\n",
      "Epoch: [19][15/37]\tLoss_mse 0.0211960\tlr: 1.0421613e-05\t\n",
      "Epoch: [19][20/37]\tLoss_mse 0.0097784\tlr: 1.0187461e-05\t\n",
      "Epoch: [19][25/37]\tLoss_mse 0.0099067\tlr: 1.2287908e-05\t\n",
      "Epoch: [19][30/37]\tLoss_mse 0.0118988\tlr: 1.6671234e-05\t\n",
      "Epoch: [19][35/37]\tLoss_mse 0.0111198\tlr: 2.3229507e-05\t\n",
      "========================\n",
      "lr:2.4789e-05\n",
      "Epoch: [20][0/37]\tLoss_mse 0.0121862\tlr: 2.6427345e-05\t\n",
      "Epoch: [20][5/37]\tLoss_mse 0.0178422\tlr: 3.5747980e-05\t\n",
      "Epoch: [20][10/37]\tLoss_mse 0.0157686\tlr: 4.6773830e-05\t\n",
      "Epoch: [20][15/37]\tLoss_mse 0.0206981\tlr: 5.9233401e-05\t\n",
      "Epoch: [20][20/37]\tLoss_mse 0.0097535\tlr: 7.2819898e-05\t\n",
      "Epoch: [20][25/37]\tLoss_mse 0.0102285\tlr: 8.7198775e-05\t\n",
      "Epoch: [20][30/37]\tLoss_mse 0.0121177\tlr: 1.0201598e-04\t\n",
      "Epoch: [20][35/37]\tLoss_mse 0.0111660\tlr: 1.1690666e-04\t\n",
      "========================\n",
      "lr:1.1986e-04\n",
      "Epoch: [21][0/37]\tLoss_mse 0.0117661\tlr: 1.2280122e-04\t\n",
      "Epoch: [21][5/37]\tLoss_mse 0.0174256\tlr: 1.3718010e-04\t\n",
      "Epoch: [21][10/37]\tLoss_mse 0.0152407\tlr: 1.5076660e-04\t\n",
      "Epoch: [21][15/37]\tLoss_mse 0.0195544\tlr: 1.6322617e-04\t\n",
      "Epoch: [21][20/37]\tLoss_mse 0.0095947\tlr: 1.7425202e-04\t\n",
      "Epoch: [21][25/37]\tLoss_mse 0.0095748\tlr: 1.8357265e-04\t\n",
      "Epoch: [21][30/37]\tLoss_mse 0.0114965\tlr: 1.9095857e-04\t\n",
      "Epoch: [21][35/37]\tLoss_mse 0.0104225\tlr: 1.9622790e-04\t\n",
      "========================\n",
      "lr:1.9702e-04\n",
      "Epoch: [22][0/37]\tLoss_mse 0.0113267\tlr: 1.9771209e-04\t\n",
      "Epoch: [22][5/37]\tLoss_mse 0.0164705\tlr: 1.9981254e-04\t\n",
      "Epoch: [22][10/37]\tLoss_mse 0.0143280\tlr: 1.9957839e-04\t\n",
      "Epoch: [22][15/37]\tLoss_mse 0.0189606\tlr: 1.9701540e-04\t\n",
      "Epoch: [22][20/37]\tLoss_mse 0.0092048\tlr: 1.9218669e-04\t\n",
      "Epoch: [22][25/37]\tLoss_mse 0.0091710\tlr: 1.8521115e-04\t\n",
      "Epoch: [22][30/37]\tLoss_mse 0.0110971\tlr: 1.7626055e-04\t\n",
      "Epoch: [22][35/37]\tLoss_mse 0.0100441\tlr: 1.6555528e-04\t\n",
      "========================\n",
      "lr:1.6323e-04\n",
      "Epoch: [23][0/37]\tLoss_mse 0.0107899\tlr: 1.6083960e-04\t\n",
      "Epoch: [23][5/37]\tLoss_mse 0.0156607\tlr: 1.4812910e-04\t\n",
      "Epoch: [23][10/37]\tLoss_mse 0.0135105\tlr: 1.3435661e-04\t\n",
      "Epoch: [23][15/37]\tLoss_mse 0.0178762\tlr: 1.1986127e-04\t\n",
      "Epoch: [23][20/37]\tLoss_mse 0.0095860\tlr: 1.0500000e-04\t\n",
      "Epoch: [23][25/37]\tLoss_mse 0.0090783\tlr: 9.0138726e-05\t\n",
      "Epoch: [23][30/37]\tLoss_mse 0.0107269\tlr: 7.5643386e-05\t\n",
      "Epoch: [23][35/37]\tLoss_mse 0.0103089\tlr: 6.1870903e-05\t\n",
      "========================\n",
      "lr:5.9233e-05\n",
      "Epoch: [24][0/37]\tLoss_mse 0.0108533\tlr: 5.6641066e-05\t\n",
      "Epoch: [24][5/37]\tLoss_mse 0.0160879\tlr: 4.4444721e-05\t\n",
      "Epoch: [24][10/37]\tLoss_mse 0.0142414\tlr: 3.3739448e-05\t\n",
      "Epoch: [24][15/37]\tLoss_mse 0.0186955\tlr: 2.4788847e-05\t\n",
      "Epoch: [24][20/37]\tLoss_mse 0.0090382\tlr: 1.7813311e-05\t\n",
      "Epoch: [24][25/37]\tLoss_mse 0.0089817\tlr: 1.2984600e-05\t\n",
      "Epoch: [24][30/37]\tLoss_mse 0.0109678\tlr: 1.0421613e-05\t\n",
      "Epoch: [24][35/37]\tLoss_mse 0.0103092\tlr: 1.0187461e-05\t\n",
      "========================\n",
      "lr:1.0422e-05\n",
      "Epoch: [25][0/37]\tLoss_mse 0.0108685\tlr: 1.0749103e-05\t\n",
      "Epoch: [25][5/37]\tLoss_mse 0.0155884\tlr: 1.3772100e-05\t\n",
      "Epoch: [25][10/37]\tLoss_mse 0.0129704\tlr: 1.9041430e-05\t\n",
      "Epoch: [25][15/37]\tLoss_mse 0.0175198\tlr: 2.6427345e-05\t\n",
      "Epoch: [25][20/37]\tLoss_mse 0.0087936\tlr: 3.5747980e-05\t\n",
      "Epoch: [25][25/37]\tLoss_mse 0.0086203\tlr: 4.6773830e-05\t\n",
      "Epoch: [25][30/37]\tLoss_mse 0.0105868\tlr: 5.9233401e-05\t\n",
      "Epoch: [25][35/37]\tLoss_mse 0.0095990\tlr: 7.2819898e-05\t\n",
      "========================\n",
      "lr:7.5643e-05\n",
      "Epoch: [26][0/37]\tLoss_mse 0.0100341\tlr: 7.8495845e-05\t\n",
      "Epoch: [26][5/37]\tLoss_mse 0.0151013\tlr: 9.3093343e-05\t\n",
      "Epoch: [26][10/37]\tLoss_mse 0.0131672\tlr: 1.0798402e-04\t\n",
      "Epoch: [26][15/37]\tLoss_mse 0.0170682\tlr: 1.2280122e-04\t\n",
      "Epoch: [26][20/37]\tLoss_mse 0.0087889\tlr: 1.3718010e-04\t\n",
      "Epoch: [26][25/37]\tLoss_mse 0.0086188\tlr: 1.5076660e-04\t\n",
      "Epoch: [26][30/37]\tLoss_mse 0.0103237\tlr: 1.6322617e-04\t\n",
      "Epoch: [26][35/37]\tLoss_mse 0.0094407\tlr: 1.7425202e-04\t\n",
      "========================\n",
      "lr:1.7626e-04\n",
      "Epoch: [27][0/37]\tLoss_mse 0.0098613\tlr: 1.7819876e-04\t\n",
      "Epoch: [27][5/37]\tLoss_mse 0.0146672\tlr: 1.8677049e-04\t\n",
      "Epoch: [27][10/37]\tLoss_mse 0.0126695\tlr: 1.9332877e-04\t\n",
      "Epoch: [27][15/37]\tLoss_mse 0.0165409\tlr: 1.9771209e-04\t\n",
      "Epoch: [27][20/37]\tLoss_mse 0.0085237\tlr: 1.9981254e-04\t\n",
      "Epoch: [27][25/37]\tLoss_mse 0.0082440\tlr: 1.9957839e-04\t\n",
      "Epoch: [27][30/37]\tLoss_mse 0.0100957\tlr: 1.9701540e-04\t\n",
      "Epoch: [27][35/37]\tLoss_mse 0.0091572\tlr: 1.9218669e-04\t\n",
      "========================\n",
      "lr:1.9096e-04\n",
      "Epoch: [28][0/37]\tLoss_mse 0.0094857\tlr: 1.8964562e-04\t\n",
      "Epoch: [28][5/37]\tLoss_mse 0.0142219\tlr: 1.8185661e-04\t\n",
      "Epoch: [28][10/37]\tLoss_mse 0.0121785\tlr: 1.7217514e-04\t\n",
      "Epoch: [28][15/37]\tLoss_mse 0.0158575\tlr: 1.6083960e-04\t\n",
      "Epoch: [28][20/37]\tLoss_mse 0.0086196\tlr: 1.4812910e-04\t\n",
      "Epoch: [28][25/37]\tLoss_mse 0.0080283\tlr: 1.3435661e-04\t\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import argparse\n",
    "import torch\n",
    "from torch import double, optim, var\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYHTONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def save_model(model, model_save_address='./model'):\n",
    "    \n",
    "    modelSave1 = model_save_address + '/model.pth.tar'\n",
    "    try:\n",
    "        torch.save({'state_dict': model.state_dict(), }, modelSave1)\n",
    "    except:\n",
    "        torch.save({'state_dict': model.module.state_dict(), }, modelSave1)\n",
    "    print('Model saved!')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Parameters for training\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--continue_training\", type=bool, default=True)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=128)\n",
    "    parser.add_argument(\"--epochs\", type=int, default=100)\n",
    "    parser.add_argument(\"--learning_rate\", type=float, default=2e-4)\n",
    "    parser.add_argument(\"--print_freq\", type=int, default=5)\n",
    "    parser.add_argument(\"--train_test_ratio\", type=float, default=0.8)\n",
    "    #parser.add_argument(\"--data_load_address\", type=str, default='./channelData')\n",
    "    parser.add_argument(\"--model_save_address\", type=str, default='./model')\n",
    "    parser.add_argument(\"--gpu_list\", type=str, default='0,1')\n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "    \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu_list\n",
    "    SEED = 42\n",
    "    seed_everything(SEED)\n",
    "\n",
    "    learning_rate = args.learning_rate\n",
    "    num_workers = 4\n",
    "\n",
    "    # parameters for data\n",
    "    train_dataset, valid_dataset = load_data2()\n",
    "    model = simpleSpatailTimeNN()\n",
    "    \n",
    "    if args.continue_training:\n",
    "        model.load_state_dict({k.replace('module.',''):v for k,v in torch.load(args.model_save_address + '/model.pth.tar')['state_dict'].items()})\n",
    "        #model.load_state_dict(torch.load(args.model_save_address + '/model.pth.tar')['state_dict'])\n",
    "        \n",
    "        \n",
    "    if len(args.gpu_list.split(',')) > 1:\n",
    "        model = torch.nn.DataParallel(model).cuda()  # model.module\n",
    "    else:\n",
    "        model = model.cuda()\n",
    "\n",
    "    \n",
    "    criterion_mse = nn.MSELoss().cuda()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100, eta_min=1e-5, last_epoch=-1)\n",
    "\n",
    "    # Data loading \n",
    "    train_dataset, test_dataset = load_data2()      \n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size)\n",
    " \n",
    "    best_score = -100    \n",
    "    \n",
    "    for epoch in range(args.epochs):\n",
    "        print('========================')\n",
    "        print('lr:%.4e'%optimizer.param_groups[0]['lr'])     \n",
    "        model.train()\n",
    "        for i, ((sst, t300, ua, va), label) in enumerate(train_loader):                \n",
    "            sst = sst.cuda().float()\n",
    "            t300 = t300.cuda().float()\n",
    "            ua = ua.cuda().float()\n",
    "            va = va.cuda().float()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            label = label.cuda().float()\n",
    "            preds = model(sst, t300, ua, va)\n",
    "            loss = criterion_mse(preds,label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if epoch > 5: \n",
    "                scheduler.step() \n",
    "            \n",
    "            if i % args.print_freq == 0:\n",
    "                print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                    'Loss_mse {loss:.7f}\\t'  \n",
    "                    'lr: {lr:.7e}\\t'.format(\n",
    "                    epoch, i, len(train_loader), loss=loss.item(), lr=optimizer.param_groups[0]['lr']))\n",
    "\n",
    "        model.eval()\n",
    "        y_true, y_pred = [], []\n",
    "        with torch.no_grad():\n",
    "            for i, ((sst, t300, ua, va), label) in enumerate(test_loader):\n",
    "                sst = sst.cuda().float()\n",
    "                t300 = t300.cuda().float()\n",
    "                ua = ua.cuda().float()\n",
    "                va = va.cuda().float()\n",
    "                label = label.cuda().float()\n",
    "                preds = model(sst, t300, ua, va)\n",
    "\n",
    "                y_pred.append(preds)\n",
    "                y_true.append(label)\n",
    "\n",
    "            y_true = torch.cat(y_true, axis=0)\n",
    "            y_pred = torch.cat(y_pred, axis=0)\n",
    "            score = eval_score(y_true.cpu().detach().numpy(), y_pred.cpu().detach().numpy())\n",
    "            if score>best_score:\n",
    "                save_model(model, args.model_save_address)\n",
    "                best_score = score\n",
    "                print('Epoch: {}, Valid Score {}'.format(epoch+1,score))\n",
    "                save_model(model, args.model_save_address)\n",
    "                print('Model saved successfully')\n",
    "                \n",
    "    del model, optimizer, train_loader,test_loader\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}